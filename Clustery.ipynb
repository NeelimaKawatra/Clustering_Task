{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMfbmMUThRQpEWuQjdHYtSW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeelimaKawatra/Clustering_Task/blob/main/Clustery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Packages"
      ],
      "metadata": {
        "id": "Gk1LwUIaswna"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hptYqGOXeB4W",
        "outputId": "cda36e5c-f71b-4cfd-af70-f3dbb42b93b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Installing required packages...\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.11/dist-packages (0.17.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.11/dist-packages (0.5.9.post2)\n",
            "Requirement already satisfied: hdbscan in /usr/local/lib/python3.11/dist-packages (0.8.40)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (1.6.1)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.11/dist-packages (from bertopic) (4.67.1)\n",
            "Requirement already satisfied: llvmlite>0.36.0 in /usr/local/lib/python3.11/dist-packages (from bertopic) (0.43.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.55.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from hdbscan) (1.5.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0->bertopic) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n",
            "✅ All packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "print(\"📦 Installing required packages...\")\n",
        "!pip install pandas openpyxl bertopic sentence-transformers umap-learn hdbscan wordcloud matplotlib plotly\n",
        "\n",
        "print(\"✅ All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing_Libraries"
      ],
      "metadata": {
        "id": "8S-_Xjkjs7st"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"📚 Importing libraries...\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from google.colab import files\n",
        "import io\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import warnings\n",
        "import wordcloud\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Text preprocessing libraries\n",
        "import nltk\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "print(\"✅ All libraries imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O029yIdHs5PC",
        "outputId": "771199aa-dde9-47f3-b4c0-6cdb38a48bef"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Importing libraries...\n",
            "✅ All libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Streamlit"
      ],
      "metadata": {
        "id": "Unhj38jouw1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install streamlit pyngrok\n",
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "WkX5GOcVuvou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1dbe3c7-358d-451e-9d36-d352ba0de01c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.48.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.3.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 1s\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\n",
            "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logs folder"
      ],
      "metadata": {
        "id": "mLgIer0SuzzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content\n",
        "!touch /content/logs.txt\n"
      ],
      "metadata": {
        "id": "5W0aLmdpu7Hy"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Preprocessing"
      ],
      "metadata": {
        "id": "rnBRiPOKtYC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "# Download NLTK data (only if not already downloaded)\n",
        "@st.cache_resource\n",
        "def download_nltk_data():\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "        nltk.data.find('corpora/stopwords')\n",
        "    except LookupError:\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "    return True\n",
        "\n",
        "# Force packages to be available - we know they work from command line\n",
        "PACKAGES_AVAILABLE = True\n",
        "\n",
        "# Import packages at module level\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "@st.cache_resource\n",
        "def load_sentence_transformer(model_name):\n",
        "    \"\"\"Cache the sentence transformer model\"\"\"\n",
        "    return SentenceTransformer(model_name)\n",
        "\n",
        "def basic_text_cleaning(text):\n",
        "    \"\"\"Basic text cleaning function\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to string and lowercase\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "\n",
        "    # Remove email addresses\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def advanced_text_cleaning(text, remove_stopwords=True, remove_punctuation=True, min_length=2):\n",
        "    \"\"\"Advanced text cleaning with options\"\"\"\n",
        "    if pd.isna(text) or text == \"\":\n",
        "        return \"\"\n",
        "\n",
        "    # Ensure NLTK data is downloaded\n",
        "    download_nltk_data()\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.tokenize import word_tokenize\n",
        "\n",
        "    # Basic cleaning first\n",
        "    text = basic_text_cleaning(text)\n",
        "\n",
        "    # Remove punctuation if requested\n",
        "    if remove_punctuation:\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenize and process\n",
        "    try:\n",
        "        tokens = word_tokenize(text)\n",
        "    except:\n",
        "        # Fallback if NLTK fails\n",
        "        tokens = text.split()\n",
        "\n",
        "    # Remove stopwords if requested\n",
        "    if remove_stopwords:\n",
        "        try:\n",
        "            stop_words = set(stopwords.words('english'))\n",
        "            tokens = [token for token in tokens if token not in stop_words]\n",
        "        except:\n",
        "            # Fallback if NLTK stopwords fail\n",
        "            basic_stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them'}\n",
        "            tokens = [token for token in tokens if token not in basic_stopwords]\n",
        "\n",
        "    # Filter by minimum length\n",
        "    tokens = [token for token in tokens if len(token) >= min_length]\n",
        "\n",
        "    # Remove digits-only tokens\n",
        "    tokens = [token for token in tokens if not token.isdigit()]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def analyze_text_quality(texts):\n",
        "    \"\"\"Analyze text quality and provide statistics\"\"\"\n",
        "    valid_texts = [text for text in texts if pd.notna(text) and str(text).strip()]\n",
        "\n",
        "    if not valid_texts:\n",
        "        return {\n",
        "            'total_texts': len(texts),\n",
        "            'empty_texts': len(texts),\n",
        "            'avg_length': 0,\n",
        "            'min_length': 0,\n",
        "            'max_length': 0,\n",
        "            'unique_texts': 0,\n",
        "            'avg_words': 0,\n",
        "            'min_words': 0,\n",
        "            'max_words': 0\n",
        "        }\n",
        "\n",
        "    stats = {\n",
        "        'total_texts': len(texts),\n",
        "        'empty_texts': len(texts) - len(valid_texts),\n",
        "        'avg_length': np.mean([len(str(text)) for text in valid_texts]),\n",
        "        'min_length': min([len(str(text)) for text in valid_texts]),\n",
        "        'max_length': max([len(str(text)) for text in valid_texts]),\n",
        "        'unique_texts': len(set([str(text) for text in valid_texts]))\n",
        "    }\n",
        "\n",
        "    # Word count analysis\n",
        "    word_counts = []\n",
        "    for text in valid_texts:\n",
        "        words = str(text).split()\n",
        "        word_counts.append(len(words))\n",
        "\n",
        "    if word_counts:\n",
        "        stats['avg_words'] = np.mean(word_counts)\n",
        "        stats['min_words'] = min(word_counts)\n",
        "        stats['max_words'] = max(word_counts)\n",
        "    else:\n",
        "        stats['avg_words'] = stats['min_words'] = stats['max_words'] = 0\n",
        "\n",
        "    return stats\n",
        "\n",
        "def has_text_content(series):\n",
        "    \"\"\"Check if a column contains meaningful text content\"\"\"\n",
        "    if series.dtype == 'object':\n",
        "        # Remove null values\n",
        "        text_data = series.dropna().astype(str)\n",
        "\n",
        "        if len(text_data) == 0:\n",
        "            return False\n",
        "\n",
        "        # Check if most values have more than just numbers/single characters\n",
        "        meaningful_text = text_data[text_data.str.len() > 2].count()\n",
        "        has_words = text_data.str.contains(' ', na=False).sum()\n",
        "\n",
        "        # At least 30% should be meaningful text with spaces\n",
        "        return (meaningful_text > len(text_data) * 0.3) and (has_words > 0)\n",
        "\n",
        "    return False\n",
        "\n",
        "def get_optimal_parameters(n_texts):\n",
        "    \"\"\"Get optimal BERTopic parameters based on dataset size\"\"\"\n",
        "    if n_texts < 50:\n",
        "        return {\n",
        "            'min_cluster_size': max(3, n_texts // 15),\n",
        "            'min_samples': 2,\n",
        "            'n_neighbors': 5,\n",
        "            'n_components': 5,\n",
        "            'embedding_model': 'all-MiniLM-L6-v2'  # Smallest, fastest model\n",
        "        }\n",
        "    elif n_texts < 200:\n",
        "        return {\n",
        "            'min_cluster_size': max(5, n_texts // 25),\n",
        "            'min_samples': 3,\n",
        "            'n_neighbors': 10,\n",
        "            'n_components': 8,\n",
        "            'embedding_model': 'all-MiniLM-L6-v2'  # Still fast\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            'min_cluster_size': max(8, n_texts // 40),\n",
        "            'min_samples': 4,\n",
        "            'n_neighbors': 15,\n",
        "            'n_components': 10,\n",
        "            'embedding_model': 'all-MiniLM-L6-v2'  # Changed from mpnet to MiniLM for speed\n",
        "        }\n",
        "\n",
        "def classify_confidence(probabilities, high_threshold=0.7, low_threshold=0.3):\n",
        "    \"\"\"Classify confidence levels based on HDBSCAN probabilities\"\"\"\n",
        "    high_conf = probabilities >= high_threshold\n",
        "    medium_conf = (probabilities >= low_threshold) & (probabilities < high_threshold)\n",
        "    low_conf = probabilities < low_threshold\n",
        "\n",
        "    return high_conf, medium_conf, low_conf\n",
        "\n",
        "def create_wordcloud_for_cluster(texts, cluster_id, title):\n",
        "    \"\"\"Create word cloud for a specific cluster\"\"\"\n",
        "    try:\n",
        "        # Combine all texts in the cluster\n",
        "        cluster_text = ' '.join(texts)\n",
        "\n",
        "        # Clean text for better word cloud\n",
        "        cluster_text = re.sub(r'[^\\w\\s]', ' ', cluster_text.lower())\n",
        "        cluster_text = re.sub(r'\\s+', ' ', cluster_text)\n",
        "\n",
        "        if len(cluster_text.strip()) > 0:\n",
        "            # Create word cloud - SMALLER SIZE\n",
        "            wordcloud = WordCloud(\n",
        "                width=300,      # Changed from 800\n",
        "                height=200,     # Changed from 400\n",
        "                background_color='white',\n",
        "                max_words=30,   # Changed from 50\n",
        "                colormap='viridis',\n",
        "                relative_scaling=0.5\n",
        "            ).generate(cluster_text)\n",
        "\n",
        "            # Plot using matplotlib - SMALLER FIGURE\n",
        "            fig, ax = plt.subplots(figsize=(4, 2))  # Changed from (10, 5)\n",
        "            ax.imshow(wordcloud, interpolation='bilinear')\n",
        "            if title:  # Only show title if provided\n",
        "                ax.set_title(title, fontsize=10, fontweight='bold')  # Smaller font\n",
        "            ax.axis('off')\n",
        "\n",
        "            return fig\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        st.warning(f\"Could not generate word cloud for {title}: {str(e)}\")\n",
        "        return None\n",
        "def display_wordclouds(topic_model, topics, texts):\n",
        "    \"\"\"Display word clouds for all clusters in Streamlit\"\"\"\n",
        "    st.subheader(\"🎨 Word Clouds for Each Cluster\")\n",
        "    st.write(\"Visual representation of the most frequent words in each cluster:\")\n",
        "\n",
        "    unique_topics = set(topics)\n",
        "    topic_list = sorted([t for t in unique_topics if t != -1])\n",
        "\n",
        "    # Display word clouds in rows of 4\n",
        "    for i in range(0, len(topic_list), 4):\n",
        "        cols = st.columns(4)\n",
        "\n",
        "        for j, topic_id in enumerate(topic_list[i:i+4]):\n",
        "            with cols[j]:\n",
        "                # Get texts for this cluster\n",
        "                cluster_texts = [texts[idx] for idx, t in enumerate(topics) if t == topic_id]\n",
        "                cluster_size = len(cluster_texts)\n",
        "\n",
        "                # Get topic keywords\n",
        "                topic_words = topic_model.get_topic(topic_id)[:3]\n",
        "                keywords = [word for word, score in topic_words]\n",
        "\n",
        "                st.write(f\"**Cluster {topic_id}** ({cluster_size})\")\n",
        "                st.caption(f\"{', '.join(keywords)}\")\n",
        "\n",
        "                # Create smaller word cloud\n",
        "                fig = create_wordcloud_for_cluster(cluster_texts, topic_id, \"\")\n",
        "\n",
        "                if fig is not None:\n",
        "                    st.pyplot(fig, clear_figure=True)\n",
        "                else:\n",
        "                    st.info(\"No word cloud\")\n",
        "\n",
        "def get_user_satisfaction_choice():\n",
        "    \"\"\"Get user's satisfaction with clustering results\"\"\"\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"🎯 How do you feel about these clustering results?\")\n",
        "\n",
        "    col1, col2 = st.columns(2)\n",
        "\n",
        "    satisfaction_choice = None\n",
        "\n",
        "    with col1:\n",
        "        if st.button(\"✅ **Happy with clustering**\\nExport results as-is\",\n",
        "                    type=\"primary\",\n",
        "                    use_container_width=True,\n",
        "                    help=\"The clusters look good! Download the results.\"):\n",
        "            satisfaction_choice = \"happy\"\n",
        "\n",
        "    with col2:\n",
        "        if st.button(\"🔧 **Want manual adjustments**\\nReview and improve clusters\",\n",
        "                    use_container_width=True,\n",
        "                    help=\"I want to review and manually adjust some clusters.\"):\n",
        "            satisfaction_choice = \"manual\"\n",
        "\n",
        "    return satisfaction_choice\n",
        "    \"\"\"Classify confidence levels based on HDBSCAN probabilities\"\"\"\n",
        "    high_conf = probabilities >= high_threshold\n",
        "    medium_conf = (probabilities >= low_threshold) & (probabilities < high_threshold)\n",
        "    low_conf = probabilities < low_threshold\n",
        "\n",
        "    return high_conf, medium_conf, low_conf\n",
        "\n",
        "def run_bertopic_clustering(texts, params):\n",
        "    \"\"\"Run BERTopic clustering - removed caching to fix import issues\"\"\"\n",
        "\n",
        "    # Debug: Print dataset info\n",
        "    print(f\"Debug: Dataset size: {len(texts)}\")\n",
        "    print(f\"Debug: Sample texts: {texts[:3]}\")\n",
        "    print(f\"Debug: Params: {params}\")\n",
        "\n",
        "    # Set up UMAP with more lenient parameters for small datasets\n",
        "    umap_model = UMAP(\n",
        "        n_neighbors=min(params['n_neighbors'], len(texts)-1),  # Ensure n_neighbors < dataset size\n",
        "        n_components=min(params['n_components'], len(texts)-1),\n",
        "        min_dist=0.0,\n",
        "        metric='cosine',\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Set up HDBSCAN with more lenient parameters\n",
        "    hdbscan_model = HDBSCAN(\n",
        "        min_cluster_size=max(2, min(params['min_cluster_size'], len(texts)//3)),  # More flexible\n",
        "        min_samples=max(1, min(params['min_samples'], len(texts)//5)),\n",
        "        metric='euclidean',\n",
        "        cluster_selection_method='eom'\n",
        "    )\n",
        "\n",
        "    # Set up embedding model using cached function\n",
        "    embedding_model = load_sentence_transformer(params['embedding_model'])\n",
        "\n",
        "    # Create BERTopic model with error handling\n",
        "    topic_model = BERTopic(\n",
        "        embedding_model=embedding_model,\n",
        "        umap_model=umap_model,\n",
        "        hdbscan_model=hdbscan_model,\n",
        "        verbose=True,  # Enable verbose to see what's happening\n",
        "        calculate_probabilities=True,\n",
        "        nr_topics=\"auto\"  # Let BERTopic decide number of topics\n",
        "    )\n",
        "\n",
        "    # Fit the model with error handling\n",
        "    try:\n",
        "        topics, probabilities = topic_model.fit_transform(texts)\n",
        "        print(f\"Debug: Generated {len(set(topics))} topics\")\n",
        "        print(f\"Debug: Topic distribution: {dict(zip(*np.unique(topics, return_counts=True)))}\")\n",
        "\n",
        "        # Check if clustering was successful\n",
        "        if probabilities is None or len(probabilities) == 0:\n",
        "            raise ValueError(\"No prediction data was generated\")\n",
        "\n",
        "        return topic_model, topics, probabilities\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Debug: BERTopic error: {str(e)}\")\n",
        "        # If BERTopic fails, try simpler approach\n",
        "        print(\"Debug: Trying fallback clustering...\")\n",
        "        return fallback_clustering(texts, embedding_model)\n",
        "\n",
        "def fallback_clustering(texts, embedding_model):\n",
        "    \"\"\"Fallback clustering using KMeans if BERTopic fails\"\"\"\n",
        "    from sklearn.cluster import KMeans\n",
        "    from sklearn.metrics import silhouette_score\n",
        "\n",
        "    # Generate embeddings\n",
        "    embeddings = embedding_model.encode(texts)\n",
        "\n",
        "    # Try different numbers of clusters\n",
        "    best_score = -1\n",
        "    best_k = 2\n",
        "\n",
        "    for k in range(2, min(len(texts)//2, 8)):\n",
        "        try:\n",
        "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "            cluster_labels = kmeans.fit_predict(embeddings)\n",
        "            score = silhouette_score(embeddings, cluster_labels)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_k = k\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    # Final clustering with best k\n",
        "    kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
        "    topics = kmeans.fit_predict(embeddings)\n",
        "\n",
        "    # Generate mock probabilities (distances to centroids converted to probabilities)\n",
        "    distances = kmeans.transform(embeddings)\n",
        "    min_distances = np.min(distances, axis=1)\n",
        "    max_dist = np.max(min_distances)\n",
        "    probabilities = 1 - (min_distances / max_dist)  # Closer = higher probability\n",
        "\n",
        "    # Create mock topic model\n",
        "    class MockTopicModel:\n",
        "        def __init__(self, texts, topics):\n",
        "            self.topics = topics\n",
        "            self.texts = texts\n",
        "\n",
        "        def get_topic_info(self):\n",
        "            topic_counts = dict(zip(*np.unique(topics, return_counts=True)))\n",
        "            return pd.DataFrame([\n",
        "                {'Topic': topic, 'Count': count}\n",
        "                for topic, count in topic_counts.items()\n",
        "            ])\n",
        "\n",
        "        def get_topic(self, topic_id):\n",
        "            # Return mock keywords\n",
        "            return [(\"keyword1\", 0.5), (\"keyword2\", 0.4), (\"keyword3\", 0.3)]\n",
        "\n",
        "    return MockTopicModel(texts, topics), topics, probabilities\n",
        "\n",
        "def page_upload():\n",
        "    \"\"\"Page 1: File Upload, Column Selection, and Preprocessing\"\"\"\n",
        "    st.title(\"🔍 Welcome to Clustery: Short Text Clustering\")\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    # File upload section\n",
        "    st.subheader(\"📁 Please upload your file\")\n",
        "\n",
        "    uploaded_file = st.file_uploader(\n",
        "        \"Choose your data file\",\n",
        "        type=['csv', 'xlsx', 'xls'],\n",
        "        help=\"Upload a CSV or Excel file containing your data\"\n",
        "    )\n",
        "\n",
        "    if uploaded_file is not None:\n",
        "        try:\n",
        "            # Read the file\n",
        "            if uploaded_file.name.endswith('.csv'):\n",
        "                df = pd.read_csv(uploaded_file)\n",
        "            else:\n",
        "                df = pd.read_excel(uploaded_file)\n",
        "\n",
        "            st.success(f\"✅ File uploaded successfully!\")\n",
        "\n",
        "            # Show top 5 rows\n",
        "            st.subheader(\"📋 Top 5 rows of your data:\")\n",
        "            st.dataframe(df.head(), use_container_width=True)\n",
        "\n",
        "            # Show basic file info\n",
        "            col1, col2, col3 = st.columns(3)\n",
        "            with col1:\n",
        "                st.metric(\"📊 Total Rows\", len(df))\n",
        "            with col2:\n",
        "                st.metric(\"📊 Total Columns\", len(df.columns))\n",
        "            with col3:\n",
        "                st.metric(\"💾 Memory Usage\", f\"{df.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
        "\n",
        "            st.markdown(\"---\")\n",
        "\n",
        "            # Column selection\n",
        "            st.subheader(\"🎯 Which column would you like to cluster?\")\n",
        "\n",
        "            # Identify potential text columns\n",
        "            text_columns = []\n",
        "            for col in df.columns:\n",
        "                if has_text_content(df[col]):\n",
        "                    text_columns.append(col)\n",
        "\n",
        "            if text_columns:\n",
        "                st.info(f\"💡 Recommended text columns: {', '.join(text_columns)}\")\n",
        "\n",
        "            selected_column = st.selectbox(\n",
        "                \"Select a column:\",\n",
        "                df.columns,\n",
        "                help=\"Choose the column containing the text you want to cluster\"\n",
        "            )\n",
        "\n",
        "            # Check if selected column has text content\n",
        "            if selected_column:\n",
        "                if has_text_content(df[selected_column]):\n",
        "                    # Text quality analysis\n",
        "                    st.subheader(\"📊 Text Quality Analysis\")\n",
        "\n",
        "                    original_texts = df[selected_column].dropna()\n",
        "                    original_stats = analyze_text_quality(original_texts)\n",
        "\n",
        "                    col1, col2, col3, col4 = st.columns(4)\n",
        "                    with col1:\n",
        "                        st.metric(\"Total Responses\", original_stats['total_texts'])\n",
        "                    with col2:\n",
        "                        st.metric(\"Empty/Invalid\", original_stats['empty_texts'])\n",
        "                    with col3:\n",
        "                        st.metric(\"Avg Length\", f\"{original_stats['avg_length']:.1f} chars\")\n",
        "                    with col4:\n",
        "                        st.metric(\"Avg Words\", f\"{original_stats['avg_words']:.1f}\")\n",
        "\n",
        "                    # Show sample data\n",
        "                    st.subheader(f\"📖 Sample data from '{selected_column}':\")\n",
        "                    sample_data = df[selected_column].dropna().head(5)\n",
        "                    for i, text in enumerate(sample_data, 1):\n",
        "                        st.write(f\"**{i}.** {str(text)[:150]}{'...' if len(str(text)) > 150 else ''}\")\n",
        "\n",
        "                    # Text length distribution\n",
        "                    if len(original_texts) > 0:\n",
        "                        text_lengths = [len(str(text)) for text in original_texts]\n",
        "                        fig = plt.figure(figsize=(10, 4))\n",
        "                        plt.hist(text_lengths, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "                        plt.title('Distribution of Text Lengths')\n",
        "                        plt.xlabel('Text Length (characters)')\n",
        "                        plt.ylabel('Count')\n",
        "                        st.pyplot(fig, clear_figure=True)\n",
        "\n",
        "                    st.markdown(\"---\")\n",
        "\n",
        "                    # Preprocessing section\n",
        "                    st.subheader(\"🔧 Text Preprocessing Options\")\n",
        "\n",
        "                    preprocessing_option = st.radio(\n",
        "                        \"Choose preprocessing level:\",\n",
        "                        [\n",
        "                            \"No preprocessing (use original text)\",\n",
        "                            \"Basic cleaning (URLs, emails, whitespace)\",\n",
        "                            \"Advanced cleaning (+ stopwords, punctuation)\",\n",
        "                            \"Custom preprocessing\"\n",
        "                        ],\n",
        "                        help=\"Preprocessing can improve clustering quality by removing noise\"\n",
        "                    )\n",
        "\n",
        "                    # Custom preprocessing options\n",
        "                    if preprocessing_option == \"Custom preprocessing\":\n",
        "                        st.write(\"**Custom Options:**\")\n",
        "                        col1, col2 = st.columns(2)\n",
        "                        with col1:\n",
        "                            remove_stopwords = st.checkbox(\"Remove stopwords\", value=True)\n",
        "                            remove_punctuation = st.checkbox(\"Remove punctuation\", value=True)\n",
        "                        with col2:\n",
        "                            min_length = st.slider(\"Minimum word length\", 1, 5, 2)\n",
        "\n",
        "                    # Process text based on selection\n",
        "                    if preprocessing_option == \"No preprocessing (use original text)\":\n",
        "                        processed_texts = [str(text) if pd.notna(text) else \"\" for text in df[selected_column]]\n",
        "                        preprocessing_details = \"No preprocessing applied\"\n",
        "\n",
        "                    elif preprocessing_option == \"Basic cleaning (URLs, emails, whitespace)\":\n",
        "                        processed_texts = [basic_text_cleaning(text) for text in df[selected_column]]\n",
        "                        preprocessing_details = \"Basic cleaning: URLs, emails, whitespace\"\n",
        "\n",
        "                    elif preprocessing_option == \"Advanced cleaning (+ stopwords, punctuation)\":\n",
        "                        processed_texts = [advanced_text_cleaning(text, remove_stopwords=True, remove_punctuation=True) for text in df[selected_column]]\n",
        "                        preprocessing_details = \"Advanced cleaning: URLs, emails, stopwords, punctuation, short words\"\n",
        "\n",
        "                    else:  # Custom preprocessing\n",
        "                        processed_texts = [advanced_text_cleaning(text, remove_stopwords=remove_stopwords,\n",
        "                                                                 remove_punctuation=remove_punctuation,\n",
        "                                                                 min_length=min_length) for text in df[selected_column]]\n",
        "                        preprocessing_details = f\"Custom: stopwords={remove_stopwords}, punctuation={remove_punctuation}, min_length={min_length}\"\n",
        "\n",
        "                    # Filter out empty texts\n",
        "                    original_count = len(processed_texts)\n",
        "                    processed_texts = [text.strip() for text in processed_texts if text.strip() and len(text.strip()) > 2]\n",
        "                    filtered_count = len(processed_texts)\n",
        "\n",
        "                    # Show before/after comparison\n",
        "                    if preprocessing_option != \"No preprocessing (use original text)\":\n",
        "                        st.subheader(\"🔍 Before/After Comparison\")\n",
        "\n",
        "                        processed_stats = analyze_text_quality(processed_texts)\n",
        "\n",
        "                        col1, col2 = st.columns(2)\n",
        "                        with col1:\n",
        "                            st.write(\"**Original:**\")\n",
        "                            st.metric(\"Total texts\", original_stats['total_texts'])\n",
        "                            st.metric(\"Avg length\", f\"{original_stats['avg_length']:.1f} chars\")\n",
        "                            st.metric(\"Avg words\", f\"{original_stats['avg_words']:.1f}\")\n",
        "\n",
        "                        with col2:\n",
        "                            st.write(\"**Processed:**\")\n",
        "                            st.metric(\"Valid texts\", filtered_count, f\"{filtered_count - original_stats['total_texts']:+d}\")\n",
        "                            st.metric(\"Avg length\", f\"{processed_stats['avg_length']:.1f} chars\", f\"{processed_stats['avg_length'] - original_stats['avg_length']:+.1f}\")\n",
        "                            st.metric(\"Avg words\", f\"{processed_stats['avg_words']:.1f}\", f\"{processed_stats['avg_words'] - original_stats['avg_words']:+.1f}\")\n",
        "\n",
        "                        # Show sample processed texts\n",
        "                        with st.expander(\"📄 Sample Processed Texts\"):\n",
        "                            for i, text in enumerate(processed_texts[:5], 1):\n",
        "                                st.write(f\"**{i}.** {text[:150]}{'...' if len(text) > 150 else ''}\")\n",
        "\n",
        "                    else:\n",
        "                        # For no preprocessing, just show the count\n",
        "                        st.info(f\"📊 Ready for clustering: {filtered_count} texts (removed {original_count - filtered_count} empty/short texts)\")\n",
        "\n",
        "                    st.markdown(\"---\")\n",
        "\n",
        "                    # Proceed button\n",
        "                    if len(processed_texts) >= 3:  # Minimum texts needed for clustering\n",
        "                        st.success(f\"✅ Ready for clustering with {len(processed_texts)} texts!\")\n",
        "\n",
        "                        if st.button(\"🚀 Let's do clustering!\", type=\"primary\", use_container_width=True):\n",
        "                            # Store data in session state\n",
        "                            st.session_state['survey_data'] = df\n",
        "                            st.session_state['text_column'] = selected_column\n",
        "                            st.session_state['processed_texts'] = processed_texts\n",
        "                            st.session_state['preprocessing_details'] = preprocessing_details\n",
        "                            st.session_state['original_stats'] = original_stats\n",
        "                            if preprocessing_option != \"No preprocessing (use original text)\":\n",
        "                                st.session_state['processed_stats'] = analyze_text_quality(processed_texts)\n",
        "                            st.session_state['current_page'] = 'clustering'\n",
        "\n",
        "                            st.balloons()\n",
        "                            st.success(\"🎉 Great! Moving to clustering setup...\")\n",
        "                            st.rerun()\n",
        "                    else:\n",
        "                        st.error(f\"❌ Need at least 3 texts for clustering. Current: {len(processed_texts)}\")\n",
        "                        st.write(\"Try:\")\n",
        "                        st.write(\"- Using less aggressive preprocessing\")\n",
        "                        st.write(\"- Selecting a different column\")\n",
        "                        st.write(\"- Checking your data quality\")\n",
        "\n",
        "                else:\n",
        "                    # Warning for non-text column\n",
        "                    st.error(\"⚠️ **This column doesn't have text to work on!**\")\n",
        "                    st.write(\"Please select a column that contains text responses suitable for clustering.\")\n",
        "\n",
        "                    # Show some sample data to help user understand\n",
        "                    st.write(f\"Sample data from '{selected_column}':\")\n",
        "                    sample_data = df[selected_column].dropna().head(3)\n",
        "                    for i, value in enumerate(sample_data, 1):\n",
        "                        st.write(f\"**{i}.** {value}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"❌ Error reading file: {str(e)}\")\n",
        "            st.write(\"Please make sure your file is a valid CSV or Excel file.\")\n",
        "            st.write(\"Common issues:\")\n",
        "            st.write(\"- File might be corrupted\")\n",
        "            st.write(\"- File might be too large\")\n",
        "            st.write(\"- File format might not be supported\")\n",
        "            st.write(\"- File might require specific encoding\")\n",
        "\n",
        "def page_clustering():\n",
        "    \"\"\"Page 2: BERTopic Clustering\"\"\"\n",
        "    st.title(\"🔍 Clustery: BERTopic Clustering\")\n",
        "\n",
        "    # Back button\n",
        "    if st.button(\"← Back to Upload\", help=\"Go back to file upload\"):\n",
        "        st.session_state['current_page'] = 'upload'\n",
        "        st.rerun()\n",
        "\n",
        "    # Check if packages are available\n",
        "    if not PACKAGES_AVAILABLE:\n",
        "        st.error(\"❌ **Missing required packages!**\")\n",
        "        st.write(\"Please install the following packages:\")\n",
        "        st.code(\"pip install bertopic sentence-transformers umap-learn hdbscan\")\n",
        "        st.info(\"After installation, restart this page.\")\n",
        "        return\n",
        "\n",
        "    # Get data from session state\n",
        "    if 'survey_data' not in st.session_state or 'text_column' not in st.session_state:\n",
        "        st.error(\"❌ **No data found!** Please upload data first.\")\n",
        "        if st.button(\"🔄 Go to Upload\"):\n",
        "            st.session_state['current_page'] = 'upload'\n",
        "            st.rerun()\n",
        "        return\n",
        "\n",
        "    df = st.session_state['survey_data']\n",
        "    text_column = st.session_state['text_column']\n",
        "\n",
        "    # Use processed texts if available, otherwise prepare from original\n",
        "    if 'processed_texts' in st.session_state:\n",
        "        texts = st.session_state['processed_texts']\n",
        "        preprocessing_details = st.session_state.get('preprocessing_details', 'Unknown preprocessing')\n",
        "\n",
        "        st.success(f\"✅ **Data loaded!** Using {len(texts)} processed responses from column '{text_column}'\")\n",
        "        st.info(f\"🔧 **Preprocessing applied:** {preprocessing_details}\")\n",
        "\n",
        "        # Show preprocessing summary if available\n",
        "        if 'original_stats' in st.session_state and 'processed_stats' in st.session_state:\n",
        "            with st.expander(\"📊 Preprocessing Summary\"):\n",
        "                original_stats = st.session_state['original_stats']\n",
        "                processed_stats = st.session_state['processed_stats']\n",
        "\n",
        "                col1, col2, col3 = st.columns(3)\n",
        "                with col1:\n",
        "                    st.metric(\"Original texts\", original_stats['total_texts'])\n",
        "                    st.metric(\"Processed texts\", len(texts))\n",
        "                with col2:\n",
        "                    st.metric(\"Original avg length\", f\"{original_stats['avg_length']:.1f}\")\n",
        "                    st.metric(\"Processed avg length\", f\"{processed_stats['avg_length']:.1f}\")\n",
        "                with col3:\n",
        "                    st.metric(\"Original avg words\", f\"{original_stats['avg_words']:.1f}\")\n",
        "                    st.metric(\"Processed avg words\", f\"{processed_stats['avg_words']:.1f}\")\n",
        "    else:\n",
        "        # Fallback to original processing\n",
        "        texts = df[text_column].dropna().astype(str).tolist()\n",
        "        texts = [text.strip() for text in texts if len(text.strip()) > 2]\n",
        "        st.success(f\"✅ **Data loaded!** Using {len(texts)} responses from column '{text_column}'\")\n",
        "        st.warning(\"⚠️ No preprocessing was applied. Consider going back to add preprocessing for better results.\")\n",
        "\n",
        "    # Show data summary\n",
        "    with st.expander(\"📊 Data Summary\"):\n",
        "        st.write(f\"**Total responses:** {len(texts)}\")\n",
        "        st.write(f\"**Column:** {text_column}\")\n",
        "        st.write(f\"**Average length:** {np.mean([len(text) for text in texts]):.1f} characters\")\n",
        "        if 'preprocessing_details' in st.session_state:\n",
        "            st.write(f\"**Preprocessing:** {st.session_state['preprocessing_details']}\")\n",
        "        st.write(\"**Sample responses:**\")\n",
        "        for i, text in enumerate(texts[:3]):\n",
        "            st.write(f\"{i+1}. {text[:100]}{'...' if len(text) > 100 else ''}\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    # Get optimal parameters\n",
        "    optimal_params = get_optimal_parameters(len(texts))\n",
        "\n",
        "    st.subheader(\"🔧 Clustering Configuration\")\n",
        "\n",
        "    col1, col2 = st.columns([3, 1])\n",
        "\n",
        "    with col1:\n",
        "        st.write(\"**Recommended parameters for your dataset:**\")\n",
        "        for key, value in optimal_params.items():\n",
        "            st.write(f\"- **{key.replace('_', ' ').title()}:** {value}\")\n",
        "\n",
        "    with col2:\n",
        "        use_optimal = st.radio(\n",
        "            \"Parameter choice:\",\n",
        "            [\"Use recommended\", \"Customize\"],\n",
        "            help=\"Recommended settings work best for most datasets\"\n",
        "        )\n",
        "\n",
        "    # Parameter selection\n",
        "    if use_optimal == \"Customize\":\n",
        "        st.subheader(\"⚙️ Custom Parameters\")\n",
        "\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            min_cluster_size = st.slider(\"Min Cluster Size\", 2, 20, optimal_params['min_cluster_size'])\n",
        "            n_neighbors = st.slider(\"UMAP Neighbors\", 2, 30, optimal_params['n_neighbors'])\n",
        "            embedding_model = st.selectbox(\"Embedding Model\",\n",
        "                                         ['all-MiniLM-L6-v2', 'all-mpnet-base-v2'],\n",
        "                                         index=0)\n",
        "\n",
        "        with col2:\n",
        "            min_samples = st.slider(\"Min Samples\", 1, 10, optimal_params['min_samples'])\n",
        "            n_components = st.slider(\"UMAP Components\", 2, 20, optimal_params['n_components'])\n",
        "\n",
        "        params = {\n",
        "            'min_cluster_size': min_cluster_size,\n",
        "            'min_samples': min_samples,\n",
        "            'n_neighbors': n_neighbors,\n",
        "            'n_components': n_components,\n",
        "            'embedding_model': embedding_model\n",
        "        }\n",
        "    else:\n",
        "        params = optimal_params\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    # Clustering button\n",
        "    col1, col2, col3 = st.columns([1, 2, 1])\n",
        "    with col2:\n",
        "        if st.button(\"🚀 Start BERTopic Clustering\", type=\"primary\", use_container_width=True):\n",
        "\n",
        "            progress_bar = st.progress(0)\n",
        "            status_text = st.empty()\n",
        "\n",
        "            try:\n",
        "                status_text.text(\"🔄 Initializing BERTopic...\")\n",
        "                progress_bar.progress(10)\n",
        "\n",
        "                status_text.text(\"🔄 Loading sentence transformer model...\")\n",
        "                progress_bar.progress(30)\n",
        "\n",
        "                status_text.text(\"🔄 Running clustering algorithm...\")\n",
        "                progress_bar.progress(50)\n",
        "\n",
        "                # Run clustering\n",
        "                topic_model, topics, probabilities = run_bertopic_clustering(texts, params)\n",
        "                progress_bar.progress(80)\n",
        "\n",
        "                # Store results\n",
        "                st.session_state['topic_model'] = topic_model\n",
        "                st.session_state['topics'] = topics\n",
        "                st.session_state['probabilities'] = probabilities\n",
        "                st.session_state['texts'] = texts\n",
        "                st.session_state['clustering_complete'] = True\n",
        "\n",
        "                progress_bar.progress(100)\n",
        "                status_text.text(\"✅ Clustering completed!\")\n",
        "\n",
        "                st.balloons()\n",
        "                st.success(\"🎉 **Clustering successful!**\")\n",
        "\n",
        "            except Exception as e:\n",
        "                st.error(f\"❌ **Error during clustering:** {str(e)}\")\n",
        "                st.write(\"This might be due to:\")\n",
        "                st.write(\"- Dataset too small\")\n",
        "                st.write(\"- Text responses too similar\")\n",
        "                st.write(\"- Parameter settings need adjustment\")\n",
        "                return\n",
        "\n",
        "    # Show results if clustering is complete\n",
        "    if st.session_state.get('clustering_complete', False):\n",
        "\n",
        "        st.markdown(\"---\")\n",
        "        st.header(\"📊 Clustering Results\")\n",
        "\n",
        "        topic_model = st.session_state['topic_model']\n",
        "        topics = st.session_state['topics']\n",
        "        probabilities = st.session_state['probabilities']\n",
        "        texts = st.session_state['texts']\n",
        "\n",
        "        # Basic statistics\n",
        "        unique_topics = len(set(topics))\n",
        "        outliers = sum(1 for t in topics if t == -1)\n",
        "        clustered = len(texts) - outliers\n",
        "\n",
        "        col1, col2, col3, col4 = st.columns(4)\n",
        "        with col1:\n",
        "            st.metric(\"🗂️ Total Clusters\", unique_topics - (1 if outliers > 0 else 0))\n",
        "        with col2:\n",
        "            st.metric(\"✅ Clustered\", clustered)\n",
        "        with col3:\n",
        "            st.metric(\"❓ Outliers\", outliers)\n",
        "        with col4:\n",
        "            cluster_rate = (clustered / len(texts)) * 100\n",
        "            st.metric(\"📈 Success Rate\", f\"{cluster_rate:.1f}%\")\n",
        "\n",
        "        # Confidence analysis\n",
        "        st.subheader(\"🎯 Confidence Analysis\")\n",
        "\n",
        "        high_conf, medium_conf, low_conf = classify_confidence(probabilities)\n",
        "\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "        with col1:\n",
        "            high_count = sum(high_conf)\n",
        "            high_pct = (high_count/len(probabilities)*100)\n",
        "            st.metric(\"🟢 High Confidence\", f\"{high_count}\", f\"{high_pct:.1f}%\")\n",
        "            st.caption(\"Probability ≥ 0.7\")\n",
        "\n",
        "        with col2:\n",
        "            med_count = sum(medium_conf)\n",
        "            med_pct = (med_count/len(probabilities)*100)\n",
        "            st.metric(\"🟡 Medium Confidence\", f\"{med_count}\", f\"{med_pct:.1f}%\")\n",
        "            st.caption(\"Probability 0.3-0.7\")\n",
        "\n",
        "        with col3:\n",
        "            low_count = sum(low_conf)\n",
        "            low_pct = (low_count/len(probabilities)*100)\n",
        "            st.metric(\"🔴 Low Confidence\", f\"{low_count}\", f\"{low_pct:.1f}%\")\n",
        "            st.caption(\"Probability < 0.3\")\n",
        "\n",
        "        # Topic details\n",
        "        st.subheader(\"📝 Cluster Details\")\n",
        "\n",
        "        topic_info = topic_model.get_topic_info()\n",
        "        if len(topic_info) > 0:\n",
        "            # Filter out outliers for main display\n",
        "            main_topics = topic_info[topic_info['Topic'] != -1] if -1 in topic_info['Topic'].values else topic_info\n",
        "\n",
        "            for idx, row in main_topics.iterrows():\n",
        "                topic_num = row['Topic']\n",
        "                topic_size = row['Count']\n",
        "\n",
        "                # Get topic words\n",
        "                topic_words = topic_model.get_topic(topic_num)\n",
        "                top_words = [word for word, score in topic_words[:5]]\n",
        "\n",
        "                # Get sample texts for this topic\n",
        "                topic_indices = [i for i, t in enumerate(topics) if t == topic_num]\n",
        "                topic_texts = [texts[i] for i in topic_indices]\n",
        "                topic_probs = [probabilities[i] for i in topic_indices]\n",
        "\n",
        "                with st.expander(f\"📋 **Cluster {topic_num}** ({topic_size} responses) - {', '.join(top_words[:3])}\"):\n",
        "\n",
        "                    col1, col2 = st.columns([2, 1])\n",
        "\n",
        "                    with col1:\n",
        "                        st.write(\"**🔤 Top Keywords:**\")\n",
        "                        st.write(\", \".join(top_words))\n",
        "\n",
        "                        st.write(\"**📄 Sample Responses:**\")\n",
        "                        # Show top 5 responses with highest confidence\n",
        "                        sorted_samples = sorted(zip(topic_texts, topic_probs), key=lambda x: x[1], reverse=True)\n",
        "                        for i, (text, prob) in enumerate(sorted_samples[:5]):\n",
        "                            confidence_emoji = \"🟢\" if prob >= 0.7 else \"🟡\" if prob >= 0.3 else \"🔴\"\n",
        "                            st.write(f\"{confidence_emoji} {text} *(conf: {prob:.2f})*\")\n",
        "\n",
        "                    with col2:\n",
        "                        avg_confidence = np.mean(topic_probs)\n",
        "                        st.metric(\"Avg Confidence\", f\"{avg_confidence:.2f}\")\n",
        "\n",
        "                        high_conf_in_topic = sum(1 for p in topic_probs if p >= 0.7)\n",
        "                        st.metric(\"High Confidence Items\", high_conf_in_topic)\n",
        "\n",
        "        # Display word clouds\n",
        "        display_wordclouds(topic_model, topics, texts)\n",
        "\n",
        "        # User satisfaction choice\n",
        "        satisfaction_choice = get_user_satisfaction_choice()\n",
        "\n",
        "        if satisfaction_choice == \"happy\":\n",
        "            st.success(\"🎉 Great! Preparing your results for export...\")\n",
        "\n",
        "            # Create results DataFrame\n",
        "            results_df = pd.DataFrame({\n",
        "                'text': texts,\n",
        "                'cluster': topics,\n",
        "                'confidence': probabilities\n",
        "            })\n",
        "\n",
        "            # Add cluster labels\n",
        "            cluster_labels = {}\n",
        "            for topic_num in set(topics):\n",
        "                if topic_num != -1:\n",
        "                    words = topic_model.get_topic(topic_num)[:3]\n",
        "                    cluster_labels[topic_num] = \"_\".join([word for word, score in words])\n",
        "                else:\n",
        "                    cluster_labels[topic_num] = \"outlier\"\n",
        "\n",
        "            results_df['cluster_label'] = results_df['cluster'].map(cluster_labels)\n",
        "\n",
        "            st.session_state['final_results'] = results_df\n",
        "\n",
        "            col1, col2, col3 = st.columns([1, 2, 1])\n",
        "            with col2:\n",
        "                st.download_button(\n",
        "                    \"📥 Download Results CSV\",\n",
        "                    results_df.to_csv(index=False),\n",
        "                    \"clustering_results.csv\",\n",
        "                    \"text/csv\",\n",
        "                    type=\"primary\",\n",
        "                    use_container_width=True\n",
        "                )\n",
        "\n",
        "        elif satisfaction_choice == \"manual\":\n",
        "            st.info(\"🔧 Proceeding to manual review configuration...\")\n",
        "            st.session_state['current_page'] = 'review'\n",
        "            st.rerun()\n",
        "\n",
        "        # Original buttons (still available)\n",
        "        st.markdown(\"---\")\n",
        "        st.subheader(\"🔄 Other Options\")\n",
        "\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "\n",
        "        with col1:\n",
        "            if st.button(\"✅ **Quick Export**\", use_container_width=True):\n",
        "                # Create results DataFrame\n",
        "                results_df = pd.DataFrame({\n",
        "                    'text': texts,\n",
        "                    'cluster': topics,\n",
        "                    'confidence': probabilities\n",
        "                })\n",
        "\n",
        "                # Add cluster labels\n",
        "                cluster_labels = {}\n",
        "                for topic_num in set(topics):\n",
        "                    if topic_num != -1:\n",
        "                        words = topic_model.get_topic(topic_num)[:3]\n",
        "                        cluster_labels[topic_num] = \"_\".join([word for word, score in words])\n",
        "                    else:\n",
        "                        cluster_labels[topic_num] = \"outlier\"\n",
        "\n",
        "                results_df['cluster_label'] = results_df['cluster'].map(cluster_labels)\n",
        "\n",
        "                st.session_state['final_results'] = results_df\n",
        "                st.success(\"🎉 Results ready for export!\")\n",
        "                st.download_button(\n",
        "                    \"📥 Download Results CSV\",\n",
        "                    results_df.to_csv(index=False),\n",
        "                    \"clustering_results.csv\",\n",
        "                    \"text/csv\"\n",
        "                )\n",
        "\n",
        "        with col2:\n",
        "            if st.button(\"🔧 **Advanced Review**\", use_container_width=True):\n",
        "                st.session_state['current_page'] = 'review'\n",
        "                st.rerun()\n",
        "\n",
        "        with col3:\n",
        "            if st.button(\"🔄 **Start over**\", use_container_width=True):\n",
        "                # Clear session state\n",
        "                for key in list(st.session_state.keys()):\n",
        "                    del st.session_state[key]\n",
        "                st.session_state['current_page'] = 'upload'\n",
        "                st.rerun()\n",
        "\n",
        "def page_review():\n",
        "    \"\"\"Page 3: Manual Review Configuration\"\"\"\n",
        "    page_confidence_review()\n",
        "\n",
        "def page_manual_review():\n",
        "    \"\"\"Page 4: Actual Manual Review\"\"\"\n",
        "    pass  # Already implemented above\")\n",
        "    st.write(\"- 🏷️ Rename clusters\")\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(page_title=\"Clustery\", layout=\"wide\")\n",
        "\n",
        "    # Initialize session state\n",
        "    if 'current_page' not in st.session_state:\n",
        "        st.session_state['current_page'] = 'upload'\n",
        "\n",
        "    # Navigation\n",
        "    current_page = st.session_state.get('current_page', 'upload')\n",
        "\n",
        "    if current_page == 'upload':\n",
        "        page_upload()\n",
        "    elif current_page == 'clustering':\n",
        "        page_clustering()\n",
        "    elif current_page == 'review':\n",
        "        page_review()\n",
        "    elif current_page == 'manual_review':\n",
        "        page_manual_review()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBhxpTOQtIsi",
        "outputId": "57642bfd-0b57-402b-8ff8-9a6a9663ba10"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install tools for creating public URL access to Streamlit\n",
        "!pip install pyngrok\n",
        "!npm install localtunnel\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctboSR24hbzZ",
        "outputId": "2eaff49c-5db1-4b63-86d5-74b704137525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 793ms\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
            "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kH1Ai7Na2CE1",
        "outputId": "bd62835d-ea91-4ca9-dcf1-ec9069cf6554"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.143.163.48\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://late-phones-play.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget -q -O - ipv4.icanhazip.com\n",
        "#!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "xumU_LA23HyS"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gY-b7gfk4BP8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}